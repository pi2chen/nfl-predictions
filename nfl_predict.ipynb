{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('Date')\n",
    "# df.drop(columns=['Unnamed: 0'])\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Team1Won</th>\n",
       "      <th>Season</th>\n",
       "      <th>Date</th>\n",
       "      <th>Team1</th>\n",
       "      <th>Team2</th>\n",
       "      <th>Home</th>\n",
       "      <th>...</th>\n",
       "      <th>Team1RushAtt_avg</th>\n",
       "      <th>Team2RushAtt_avg</th>\n",
       "      <th>Team1RushYds_avg</th>\n",
       "      <th>Team2RushYds_avg</th>\n",
       "      <th>Team1RYM_avg</th>\n",
       "      <th>Team2RYM_avg</th>\n",
       "      <th>Team1PYM_avg</th>\n",
       "      <th>Team2PYM_avg</th>\n",
       "      <th>Team1YM_avg</th>\n",
       "      <th>Team2YM_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1459</td>\n",
       "      <td>False</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-09-11</td>\n",
       "      <td>DAL</td>\n",
       "      <td>TAM</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>71.0</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>-81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>-22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>1383</td>\n",
       "      <td>True</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-09-26</td>\n",
       "      <td>DAL</td>\n",
       "      <td>NYG</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>71.0</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>-81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>-22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>1359</td>\n",
       "      <td>True</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>DAL</td>\n",
       "      <td>WAS</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>123.5</td>\n",
       "      <td>159.500000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>-12.000000</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>1341</td>\n",
       "      <td>True</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-10-09</td>\n",
       "      <td>DAL</td>\n",
       "      <td>LAR</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.666667</td>\n",
       "      <td>28.333333</td>\n",
       "      <td>103.0</td>\n",
       "      <td>153.666667</td>\n",
       "      <td>-50.666667</td>\n",
       "      <td>50.666667</td>\n",
       "      <td>28.666667</td>\n",
       "      <td>-28.666667</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>1309</td>\n",
       "      <td>False</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-10-16</td>\n",
       "      <td>DAL</td>\n",
       "      <td>PHI</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>124.750000</td>\n",
       "      <td>-6.750000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>-30.750000</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>-37.5</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    level_0  index  Unnamed: 0    ID  Team1Won  Season        Date Team1  \\\n",
       "6         6      6           6  1459     False    2022  2022-09-11   DAL   \n",
       "47       47     47          47  1383      True    2022  2022-09-26   DAL   \n",
       "53       53     53          53  1359      True    2022  2022-10-02   DAL   \n",
       "74       74     74          74  1341      True    2022  2022-10-09   DAL   \n",
       "88       88     88          88  1309     False    2022  2022-10-16   DAL   \n",
       "\n",
       "   Team2  Home  ...  Team1RushAtt_avg  Team2RushAtt_avg  Team1RushYds_avg  \\\n",
       "6    TAM     1  ...         18.000000         33.000000              71.0   \n",
       "47   NYG     0  ...         18.000000         33.000000              71.0   \n",
       "53   WAS     1  ...         24.000000         29.000000             123.5   \n",
       "74   LAR     0  ...         25.666667         28.333333             103.0   \n",
       "88   PHI     0  ...         27.750000         25.000000             118.0   \n",
       "\n",
       "    Team2RushYds_avg  Team1RYM_avg  Team2RYM_avg  Team1PYM_avg  Team2PYM_avg  \\\n",
       "6         152.000000    -81.000000     81.000000    -22.000000     22.000000   \n",
       "47        152.000000    -81.000000     81.000000    -22.000000     22.000000   \n",
       "53        159.500000    -36.000000     36.000000     12.000000    -12.000000   \n",
       "74        153.666667    -50.666667     50.666667     28.666667    -28.666667   \n",
       "88        124.750000     -6.750000      6.750000    -30.750000     30.750000   \n",
       "\n",
       "    Team1YM_avg  Team2YM_avg  \n",
       "6        -103.0        103.0  \n",
       "47       -103.0        103.0  \n",
       "53        -24.0         24.0  \n",
       "74        -22.0         22.0  \n",
       "88        -37.5         37.5  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Team1'] == 'DAL'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the expanding averages over previous games. \n",
    "\n",
    "numeric_columns = ['Home',\n",
    "\t\t\t\t'Team1Pts',    \n",
    "\t\t\t\t'Team2Pts',    \n",
    "\t\t\t\t'Team1PtDiff',\n",
    "\t\t\t\t'Team2PtDiff', \n",
    "\t\t\t\t'Team1TM',    \n",
    "\t\t\t\t'Team2TM',     \n",
    "\t\t\t\t'Team1Rating', \n",
    "\t\t\t\t'Team2Rating', \n",
    "\t\t\t\t'Team1Sks',    \n",
    "\t\t\t\t'Team2Sks',    \n",
    "\t\t\t\t'Team1SkYds',  \n",
    "\t\t\t\t'Team2SkYds',  \n",
    "\t\t\t\t'Team1RushAtt',\n",
    "\t\t\t\t'Team2RushAtt',\n",
    "\t\t\t\t'Team1RushYds',\n",
    "\t\t\t\t'Team2RushYds',\n",
    "\t\t\t\t'Team1RYM',    \n",
    "\t\t\t\t'Team2RYM',    \n",
    "\t\t\t\t'Team1PYM',    \n",
    "\t\t\t\t'Team2PYM',    \n",
    "\t\t\t\t'Team1YM',     \n",
    "\t\t\t\t'Team2YM']\n",
    "\n",
    "for column in numeric_columns:\n",
    "\tavg_col_name = column + '_avg'\n",
    "\tdf[avg_col_name] = (\n",
    "\t\tdf.groupby('Team1', group_keys=False)[column]\n",
    "\t\t.apply(lambda group: group.expanding().mean().shift(1))\n",
    "\t\t.reset_index(drop=True)\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the integrity of our model evaluation, we will hold out a test set and set it aside for later use. Since the dataframe has already been sorted in chronological order, we can simply split it without worrying about data leakage. This chronological sorting ensures that the time series logic is preserved, allowing us to test the model on future data points that were not part of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do cross validation and training!\n",
    "\n",
    "This code defines a function, `prep_data_for_fold`, to prepare training and testing datasets for each fold in a time series cross-validation split. It starts by identifying relevant feature columns for the training and testing sets. The `pre_game_cols` represent pre-game data, while `train_post_game_cols` capture various post-game statistics (e.g., points, rushing yards, sacks, ratings). For testing, the equivalent columns are suffixed with `_avg` to indicate they contain average statistics up to the current game. The function takes a specific fold (a tuple of training and testing indices) and a dataframe as inputs. It splits the data into training and testing subsets based on these indices, extracts the specified features (`X_train` and `X_test`), and retrieves the corresponding outcome labels (`y_train` and `y_test`). These prepared datasets are then returned, ready for use in training and evaluating the model during each fold of the time series split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test sets for each fold of TimeSeriesSplit\n",
    "\n",
    "pre_game_cols = ['Team1', 'Team2', 'Home']\n",
    "train_post_game_cols = ['Team1Pts', 'Team2Pts', 'Team1RushYds', 'Team2RushYds', 'Team1SkYds', 'Team2SkYds',\n",
    "                  'Team1Sks', 'Team2Sks', 'Team1RushAtt', 'Team2RushAtt', 'Team1RYM', 'Team2RYM', \n",
    "                  'Team1PYM', 'Team2PYM', 'Team1YM', 'Team2YM', 'Team1Rating', 'Team2Rating']\n",
    "\n",
    "test_post_game_cols = [col + '_avg' for col in train_post_game_cols]\n",
    "\n",
    "outcome_col = 'Team1Won'\n",
    "\n",
    "def prep_data_for_fold(fold, df):\n",
    "  \n",
    "  # Split data into training and testing based on the fold\n",
    "  train_indices, test_indices = fold\n",
    "  train_data = df.iloc[train_indices]\n",
    "  test_data = df.iloc[test_indices]\n",
    "\n",
    "  # Extract features that will be trained and tested on\n",
    "  X_train = train_data[train_post_game_cols]\n",
    "  X_test = test_data[test_post_game_cols]\n",
    "  \n",
    "  # Class labels from fold split\n",
    "  y_train = train_data[outcome_col]\n",
    "  y_test = test_data[outcome_col]\n",
    "  \n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TODO: PCA???\n",
    "# TODO: Run SequentialFeatureSelector to only use best features?\n",
    "\n",
    "# Nested cross-validation loop that computes how well a model does.\n",
    "# Return an accuracy score averaged over all folds of the TimeSeriesSplit.\n",
    "\n",
    "def get_model_accuracy(model, params, df):\n",
    "\n",
    "  tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "  accuracies = []\n",
    "\n",
    "  # Outer loop: find average accuracy over all time series splits\n",
    "\n",
    "  for train_indices, test_indices in tscv.split(df):\n",
    "\n",
    "    # Prepare the data for this fold\n",
    "    X_train, X_test, y_train, y_test = prep_data_for_fold((train_indices, test_indices), df)\n",
    "\n",
    "    # Rename the <col>_avg columns to just <col> so GridSearchCV doesn't complain\n",
    "    X_test = X_test.rename(columns=lambda x: x[:-4] if x.endswith('_avg') else x)\n",
    "\n",
    "    # Scale the data (fit scaler on training data and transform both train and test)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Inner loop: find the best hyperparameters for this split\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=params, cv=tscv)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Analyze how well model does by comparing its predictions to actual class labels\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "  \n",
    "  print(best_model)\n",
    "\n",
    "  # Return the average accuracy across all outer folds\n",
    "  return sum(accuracies) / len(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=20, max_features=15, min_samples_leaf=5)\n",
      "0.5983739837398374\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m models_dict[svc] \u001b[38;5;241m=\u001b[39m svc_params\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m---> 67\u001b[0m   score \u001b[38;5;241m=\u001b[39m get_model_accuracy(model, models_dict[model], df)\n\u001b[0;32m     68\u001b[0m   \u001b[38;5;28mprint\u001b[39m(score)\n",
      "Cell \u001b[1;32mIn[19], line 40\u001b[0m, in \u001b[0;36mget_model_accuracy\u001b[1;34m(model, params, df)\u001b[0m\n\u001b[0;32m     37\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Analyze how well model does by comparing its predictions to actual class labels\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     41\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[0;32m     42\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "File \u001b[1;32mc:\\Users\\ExoHorizon\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:451\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    450\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 451\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    453\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ExoHorizon\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:432\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    429\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    430\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 432\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    433\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\ExoHorizon\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\ExoHorizon\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\ExoHorizon\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ExoHorizon\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "models_dict = {}\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc_params = {\n",
    "  'max_depth': [5, 10, 15, 20],\n",
    "  'max_features': [5, 10, 15],\n",
    "  'min_samples_leaf': [5, 10, 15, 20]\n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr_params = {\n",
    "\t'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "\t'C': [0.1, 1, 10],\n",
    "\t'solver': ['liblinear', 'saga'],\n",
    "\t'max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_params = {\n",
    "  'n_estimators': [50, 100, 200],\n",
    "  'max_depth': [None, 10, 20],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_params = {\n",
    "  'n_estimators': [50, 100, 200],\n",
    "  'learning_rate': [0.01, 0.1, 0.2],\n",
    "  'max_depth': [3, 5, 7],\n",
    "  'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_params = {\n",
    "  'n_neighbors': [3, 5, 10],\n",
    "  'weights': ['uniform', 'distance'],\n",
    "  'p': [1, 2]  # 1 = Manhattan distance, 2 = Euclidean distance\n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "svc_params = {\n",
    "  'C': [0.1, 1, 10],\n",
    "  'kernel': ['linear', 'rbf', 'poly'],\n",
    "  'gamma': ['scale', 'auto'],\n",
    "  'degree': [2, 3, 4]  # Only for 'poly' kernel\n",
    "}\n",
    "\n",
    "models_dict[dtc] = dtc_params\n",
    "models_dict[lr] = lr_params\n",
    "# models_dict[rfc] = rfc_params\n",
    "# models_dict[gbc] = gbc_params\n",
    "models_dict[knn] = knn_params\n",
    "models_dict[svc] = svc_params\n",
    "\n",
    "for model in models_dict.keys():\n",
    "  score = get_model_accuracy(model, models_dict[model], df)\n",
    "  print(score)\n",
    "\n",
    "# TODO: Handle <col>_avg NaN for each team's first game of the season\n",
    "# LogisticRegression() does not accept missing values encoded as NaN natively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a final evaluation of the model on the held out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.581081081081081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Let's say that LogisticRegression(C=0.1, penalty='l1', solver='saga') is the best model.\n",
    "\n",
    "lr = LogisticRegression(C=0.1, solver='saga', penalty='l1')\n",
    "\n",
    "X_train = train_set[train_post_game_cols]\n",
    "X_test = test_set[test_post_game_cols].rename(columns=lambda x: x[:-4] if x.endswith('_avg') else x)\n",
    "y_train = train_set[outcome_col]\n",
    "y_test = test_set[outcome_col]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TP = conf_matrix[0, 0]\n",
    "FN = conf_matrix[0, 1]\n",
    "FP = conf_matrix[1, 0]\n",
    "TN = conf_matrix[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(TP, FN, FP, TN):\n",
    "    table_data = [[TP,FN],[FP,TN]]\n",
    "    df = pd.DataFrame(table_data, columns =['Predicted 1','Predicted 0'])\n",
    "    df = df.rename(index={0: 'Actual 1', 1: 'Actual 0'})\n",
    "    display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 1</th>\n",
       "      <th>Predicted 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 1  Predicted 0\n",
       "Actual 1           43           24\n",
       "Actual 0           38           43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_confusion_matrix(TP, FN, FP, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
