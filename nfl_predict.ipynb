{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('nfl-cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('Date')\n",
    "# df.drop(columns=['Unnamed: 0'])\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Testing Our Model: Expanding Mean Statistics**  \n",
    "To effectively train and test our model, we must compute expanding averages over the statistics from previous games. This approach ensures that the predictions for an upcoming game are based solely on data available prior to that game. By using expanding averages rather than rolling averages, we capture the cumulative mean of all prior games, rather than limiting the calculation to a fixed sliding window. This method provides a more comprehensive view of a teamâ€™s performance trajectory throughout the season, offering a robust foundation for predicting the outcome of the next game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Team1'] == 'DAL'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the expanding averages over previous games. \n",
    "\n",
    "numeric_columns = ['Home',\n",
    "\t\t\t\t'Team1Pts',    \n",
    "\t\t\t\t'Team2Pts',    \n",
    "\t\t\t\t'Team1PtDiff',\n",
    "\t\t\t\t'Team2PtDiff', \n",
    "\t\t\t\t'Team1TM',    \n",
    "\t\t\t\t'Team2TM',     \n",
    "\t\t\t\t'Team1Rating', \n",
    "\t\t\t\t'Team2Rating', \n",
    "\t\t\t\t'Team1Sks',    \n",
    "\t\t\t\t'Team2Sks',    \n",
    "\t\t\t\t'Team1SkYds',  \n",
    "\t\t\t\t'Team2SkYds',  \n",
    "\t\t\t\t'Team1RushAtt',\n",
    "\t\t\t\t'Team2RushAtt',\n",
    "\t\t\t\t'Team1RushYds',\n",
    "\t\t\t\t'Team2RushYds',\n",
    "\t\t\t\t'Team1RYM',    \n",
    "\t\t\t\t'Team2RYM',    \n",
    "\t\t\t\t'Team1PYM',    \n",
    "\t\t\t\t'Team2PYM',    \n",
    "\t\t\t\t'Team1YM',     \n",
    "\t\t\t\t'Team2YM']\n",
    "\n",
    "for column in numeric_columns:\n",
    "\tavg_col_name = column + '_avg'\n",
    "\tdf[avg_col_name] = (\n",
    "\t\tdf.groupby('Team1', group_keys=False)[column]\n",
    "\t\t.apply(lambda group: group.expanding().mean().shift(1))\n",
    "\t\t.reset_index(drop=True)\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that expanding mean statistics have been computed.\n",
    "\n",
    "df[df['Team1'] == 'DAL'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Should we drop the first column or impute with its original values?\n",
    "\n",
    "for column in numeric_columns:\n",
    "\tavg_col_name = column + '_avg'\n",
    "\tdf[avg_col_name] = df[avg_col_name].fillna(df[column])\n",
    "\n",
    "df[df['Team1'] == 'DAL'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the integrity of our model evaluation, we will hold out a test set and set it aside for later use. Since the dataframe has already been sorted in chronological order, we can simply split it without worrying about data leakage. This chronological sorting ensures that the time series logic is preserved, allowing us to test the model on future data points that were not part of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_set, test_set = np.split(df, [int(0.8 * len(df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do cross validation and training!\n",
    "\n",
    "This code defines a function, `prep_data_for_fold`, to prepare training and testing datasets for each fold in a time series cross-validation split. It starts by identifying relevant feature columns for the training and testing sets. The `pre_game_cols` represent pre-game data, while `train_post_game_cols` capture various post-game statistics (e.g., points, rushing yards, sacks, ratings). For testing, the equivalent columns are suffixed with `_avg` to indicate they contain average statistics up to the current game. The function takes a specific fold (a tuple of training and testing indices) and a dataframe as inputs. It splits the data into training and testing subsets based on these indices, extracts the specified features (`X_train` and `X_test`), and retrieves the corresponding outcome labels (`y_train` and `y_test`). These prepared datasets are then returned, ready for use in training and evaluating the model during each fold of the time series split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test sets for each fold of TimeSeriesSplit\n",
    "\n",
    "pre_game_cols = ['Team1', 'Team2', 'Home']\n",
    "train_post_game_cols = ['Team1Pts', 'Team2Pts', 'Team1RushYds', 'Team2RushYds', 'Team1SkYds', 'Team2SkYds',\n",
    "                  'Team1Sks', 'Team2Sks', 'Team1RushAtt', 'Team2RushAtt', 'Team1RYM', 'Team2RYM', \n",
    "                  'Team1PYM', 'Team2PYM', 'Team1YM', 'Team2YM', 'Team1Rating', 'Team2Rating']\n",
    "\n",
    "test_post_game_cols = [col + '_avg' for col in train_post_game_cols]\n",
    "\n",
    "outcome_col = 'Team1Won'\n",
    "\n",
    "def prep_data_for_fold(fold, df):\n",
    "  \n",
    "  # Split data into training and testing based on the fold\n",
    "  train_indices, test_indices = fold\n",
    "  train_data = df.iloc[train_indices]\n",
    "  test_data = df.iloc[test_indices]\n",
    "\n",
    "  # Extract features that will be trained and tested on\n",
    "  X_train = train_data[train_post_game_cols]\n",
    "  X_test = test_data[test_post_game_cols]\n",
    "  \n",
    "  # Class labels from fold split\n",
    "  y_train = train_data[outcome_col]\n",
    "  y_test = test_data[outcome_col]\n",
    "  \n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TODO: PCA???\n",
    "# TODO: Run SequentialFeatureSelector to only use best features?\n",
    "\n",
    "# Nested cross-validation loop that computes how well a model does.\n",
    "# Return an accuracy score averaged over all folds of the TimeSeriesSplit.\n",
    "\n",
    "def get_model_accuracy(model, params, df):\n",
    "\n",
    "  tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "  accuracies = []\n",
    "\n",
    "  # Outer loop: find average accuracy over all time series splits\n",
    "\n",
    "  for train_indices, test_indices in tscv.split(df):\n",
    "\n",
    "    # Prepare the data for this fold\n",
    "    X_train, X_test, y_train, y_test = prep_data_for_fold((train_indices, test_indices), df)\n",
    "\n",
    "    # Rename the <col>_avg columns to just <col> so GridSearchCV doesn't complain\n",
    "    X_test = X_test.rename(columns=lambda x: x[:-4] if x.endswith('_avg') else x)\n",
    "\n",
    "    # Scale the data (fit scaler on training data and transform both train and test)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Inner loop: find the best hyperparameters for this split\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=params, cv=tscv)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model from grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Analyze how well model does by comparing its predictions to actual class labels\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "  \n",
    "  print(best_model)\n",
    "\n",
    "  # Return the average accuracy across all outer folds\n",
    "  return sum(accuracies) / len(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=20, max_features=15, min_samples_leaf=15)\n",
      "0.5853658536585366\n",
      "LogisticRegression(C=0.1, penalty='l1', solver='saga')\n",
      "0.6097560975609756\n",
      "KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
      "0.5772357723577235\n",
      "SVC(C=1, degree=2, kernel='linear')\n",
      "0.6032520325203252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "models_dict = {}\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc_params = {\n",
    "  'max_depth': [5, 10, 15, 20],\n",
    "  'max_features': [5, 10, 15],\n",
    "  'min_samples_leaf': [5, 10, 15, 20]\n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr_params = {\n",
    "\t'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "\t'C': [0.1, 1, 10],\n",
    "\t'solver': ['liblinear', 'saga'],\n",
    "\t'max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_params = {\n",
    "  'n_estimators': [50, 100, 200],\n",
    "  'max_depth': [None, 10, 20],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_params = {\n",
    "  'n_estimators': [50, 100, 200],\n",
    "  'learning_rate': [0.01, 0.1, 0.2],\n",
    "  'max_depth': [3, 5, 7],\n",
    "  'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_params = {\n",
    "  'n_neighbors': [3, 5, 10],\n",
    "  'weights': ['uniform', 'distance'],\n",
    "  'p': [1, 2]  # 1 = Manhattan distance, 2 = Euclidean distance\n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "svc_params = {\n",
    "  'C': [0.1, 1, 10],\n",
    "  'kernel': ['linear', 'rbf', 'poly'],\n",
    "  'gamma': ['scale', 'auto'],\n",
    "  'degree': [2, 3, 4]  # Only for 'poly' kernel\n",
    "}\n",
    "\n",
    "models_dict[dtc] = dtc_params\n",
    "models_dict[lr] = lr_params\n",
    "# models_dict[rfc] = rfc_params\n",
    "# models_dict[gbc] = gbc_params\n",
    "models_dict[knn] = knn_params\n",
    "models_dict[svc] = svc_params\n",
    "\n",
    "for model in models_dict.keys():\n",
    "  score = get_model_accuracy(model, models_dict[model], df)\n",
    "  print(score)\n",
    "\n",
    "# TODO: Handle <col>_avg NaN for each team's first game of the season\n",
    "# LogisticRegression() does not accept missing values encoded as NaN natively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a final evaluation of the model on the held out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.581081081081081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Let's say that LogisticRegression(C=0.1, penalty='l1', solver='saga') is the best model.\n",
    "\n",
    "lr = LogisticRegression(C=0.1, solver='saga', penalty='l1')\n",
    "\n",
    "X_train = train_set[train_post_game_cols]\n",
    "X_test = test_set[test_post_game_cols].rename(columns=lambda x: x[:-4] if x.endswith('_avg') else x)\n",
    "y_train = train_set[outcome_col]\n",
    "y_test = test_set[outcome_col]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TP = conf_matrix[0, 0]\n",
    "FN = conf_matrix[0, 1]\n",
    "FP = conf_matrix[1, 0]\n",
    "TN = conf_matrix[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(TP, FN, FP, TN):\n",
    "    table_data = [[TP,FN],[FP,TN]]\n",
    "    df = pd.DataFrame(table_data, columns =['Predicted 1','Predicted 0'])\n",
    "    df = df.rename(index={0: 'Actual 1', 1: 'Actual 0'})\n",
    "    display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 1</th>\n",
       "      <th>Predicted 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 1  Predicted 0\n",
       "Actual 1           43           24\n",
       "Actual 0           38           43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_confusion_matrix(TP, FN, FP, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
