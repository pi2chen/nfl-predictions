{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Introduction**\n",
    "\n",
    "The goal of this project is to predict which NFL team will win a match based on their recent historical performance. This is a classification problem, where we aim to forecast the outcome (win or loss) of a game using historical team data and performance metrics.\n",
    "\n",
    "Predicting NFL game outcomes benefits:\n",
    "- **Coaches** by refining strategies,\n",
    "- **Analysts** by providing insights,\n",
    "- **Betting companies** by setting better odds, and\n",
    "- **Fans** by boosting engagement.\n",
    "\n",
    "By analyzing historical performance data, we can uncover patterns that inform decision-making for team preparation, betting strategies, and fan interactions. This improves outcomes for all stakeholders involved, from optimizing team tactics to generating content for fans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in base data from csv\n",
    "df = pd.read_csv('nfl.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Cleaning & Exploration**\n",
    "Here is the general overview of the steps we took during data cleaning & exploration.\n",
    "\n",
    "- **Handle missing/invalid values**: Fill in or remove missin/invalid data to maintain dataset integrity.\n",
    "- **Standardize formats & data types & units**: Ensure consistency in date formats, data types, and measurement units.\n",
    "- **Remove unnecessary columns**: Drop irrelevant columns to simplify the dataset.\n",
    "- **Anomoly detection & filter outliers**: Identify and manage extreme values that may distort analysis.\n",
    "- **Correlation analysis**: Analyze relationships between variables to identify strong correlations that to inform feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Handle Missing/Invalid Values**\n",
    "To handle missing values, we count the number of null values in each column.\n",
    "- We can see that the data is quite complete, with no null values except in the away column.\n",
    "- Upon further inspection of the away column, it can be seen that the NaN value is used to denote that the current team is at home and the @ value is used to denote that the current team is away.\n",
    "- Therefore, we do not need to handle any missing values.\n",
    "\n",
    "Upon further data inspection, we found that some games ended in a tie.\n",
    "- We will be considering this an invalid type since we are only trying to predict a win or loss\n",
    "- Therefore, all rows which have a tie result will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "if (null_counts > 0).any():\n",
    "    print(null_counts[null_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle invalid values\n",
    "\n",
    "df = df[~df['Result'].str[0].isin(['T'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Standardize Formats, Data Types, & Units**\n",
    "\n",
    "Before we complete the other steps in data cleaning, we should perform data standardization to make comparisons easier. This includes:\n",
    "- Check that each column has a consistent type.\n",
    "  - Our check found that each column had a consitent type except 'Away', which we handle in the next step.\n",
    "- Standardizing non-numeric / categorical data.\n",
    "  - Here we print out any columns without the 'int64' or 'float64' type.\n",
    "  - Then we go through each column and turn those into a standardized type.\n",
    "  - As a final check, we print the data type of each column and double check by inspecing the data.\n",
    "- Confirming consistent units.\n",
    "  - Since the source of the data provides units, they are expected to be consistent.\n",
    "  - However, this is further checked by a visual inspection of the data and any outliers/anomolies which may be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that each column has a consistent type\n",
    "for column in df.columns:\n",
    "  if df[column].map(type).nunique() > 1:\n",
    "    print(f\"Column '{column}' contains mixed types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing non-numeric data\n",
    "non_numeric_columns = df.select_dtypes(exclude=['int64', 'float64'])\n",
    "print(non_numeric_columns.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing non-numeric data\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "def convert_to_seconds(time_str):\n",
    "    minutes, seconds = map(int, time_str.split(':'))\n",
    "    return minutes * 60 + seconds\n",
    "df['ToP'] = df['ToP'].apply(convert_to_seconds)\n",
    "df['ToP.1'] = df['ToP.1'].apply(convert_to_seconds)\n",
    "\n",
    "df['Away'] = df['Away'].isna().astype(int)\n",
    "\n",
    "def convert_to_minutes(time_str):\n",
    "    return int(time_str.split(':')[0])\n",
    "df['Time'] = df['Time'].apply(convert_to_minutes)\n",
    "\n",
    "df['Result'] = df['Result'].str[0].map({'W': 1, 'L': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing non-numeric data\n",
    "non_numeric_columns = df.select_dtypes(exclude=['int64', 'float64'])\n",
    "print(non_numeric_columns.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing non-numeric data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Remove Unnecessary Columns**\n",
    "\n",
    "To simplify our dataset, we should look to see if there are any unncessary columns. This can include:\n",
    "- Columns containing irrelavent information\n",
    "  - Upon inspection of the data, we can see that the 'Rk' column served as an index column. \n",
    "  - Therefore, the 'Rk' column can be considered to contain irrelavent information to out model and thus removed.\n",
    "- Columns with constant data\n",
    "  - No columns were found to have constant data\n",
    "- Columns with excessive missing data\n",
    "  - Since we have handled and discovered no missing values, we do not need to account for this case\n",
    "- Columns with identical data (we will keep the first column alphabetically for each duplicate)\n",
    "  - Column 'Att' is a duplicate of column 'Rush_Att'\n",
    "  - Column 'Att.1' is a duplicate of column 'Opp_Rush_Att'\n",
    "  - Column 'DY/P' is a duplicate of column 'DY/P.1'\n",
    "  - Column 'Pts' is a duplicate of column 'Pts.1'\n",
    "  - Column 'PtsO' is a duplicate of column 'PtsO.1'\n",
    "  - Column 'Rate' is a duplicate of column 'Rate.2'\n",
    "  - Column 'Rate.1' is a duplicate of column 'Rate.3'\n",
    "  - Column 'TO' is a duplicate of column 'TO.2'\n",
    "  - Column 'ToP' is a duplicate of column 'ToP.1'\n",
    "  - Column 'Y/P' is a duplicate of column 'Y/P.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns containing irrelavent information\n",
    "df.drop('Rk', axis=1, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with constant data\n",
    "constant_columns = df.columns[df.nunique() == 1]\n",
    "print('Columns with constant data:', constant_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with identical data\n",
    "duplicate_columns = {}\n",
    "seen_pairs = set()\n",
    "\n",
    "for col1 in df.columns:\n",
    "  for col2 in df.columns:\n",
    "    if col1 != col2 and df[col1].equals(df[col2]):\n",
    "      if (col1, col2) not in seen_pairs and (col2, col1) not in seen_pairs:\n",
    "        duplicate_columns[col1] = col2\n",
    "        seen_pairs.add((col1, col2))\n",
    "\n",
    "sorted_duplicate_columns = sorted(duplicate_columns.items())\n",
    "\n",
    "for col, duplicate in sorted_duplicate_columns:\n",
    "  print(f\"Column '{col}' is a duplicate of column '{duplicate}'\")\n",
    "\n",
    "duplicated_columns_list = list(set(duplicate_columns.keys()).union(duplicate_columns.values()))\n",
    "print(df[duplicated_columns_list].head(5).T.sort_index(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with identical data\n",
    "for col1, duplicate in duplicate_columns.items():\n",
    " if duplicate in df.columns:\n",
    "    df.drop(duplicate, axis=1, inplace=True)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Anomoly Detection & Filter Outliers**\n",
    "\n",
    "Anomaly detection is essential to identify and address outliers or errors in the dataset.\n",
    "This helps to ensure that the analysis remains accurate and the model is not influenced by misleading or irrelevant data points. Note that this is also somewhat part of data exploration.\n",
    "We followed the steps below:\n",
    "\n",
    "- Plot distributions for numeric data\n",
    "\t- Before we search for anomolies, is good to visualize how the data is distributed\n",
    "\t- Additionally, we should note that some of the numeric values are inheriently categorical or continous such as:\n",
    "\t\t- Result\n",
    "\t\t\tSeason\n",
    "\t\t\tTime\n",
    "\t\t\tWeek\n",
    "\t\t\tG#\n",
    "- Using the plots, we can inspect to see the types of distributions\n",
    "\t- Only only classified Yds.1 and Yds.3 as \"exponential\"\n",
    "\t- Everything else is considered to have a \"normal\" distribution, even if severly skewed\n",
    "- By separetly calculating the Z-scores for normal and exponential features and combining them at the end:\n",
    "\t- We get an anomoly rate of 15.60% for Z-score threshold of 3 and 2.92% for Z-score threshold of 4\n",
    "\t- Printing out the anomolies for Z-score threshold of 4, we see games with still relatively normal stats\n",
    "\t- Therefore, we concluded that due to the chaotic and variable nature sports, especially with football, we cannot confindelty determine which games should be considered anomolies, so no rows are removed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['Result', 'Season', 'Time', 'Week', 'G#']\n",
    "\n",
    "def get_numeric_columns(df, exclude_columns):\n",
    "    return sorted([col for col in df.select_dtypes(include=['float64', 'int64']).columns if col not in exclude_columns])\n",
    "\n",
    "numeric_columns = get_numeric_columns(df, exclude_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_distributions(data):\n",
    "\tnorm_params = stats.norm.fit(data) # Normal distribution\n",
    "\t\n",
    "\texp_params = stats.expon.fit(data) # Exponential distribution\n",
    "\t\n",
    "\tif (data > 0).all():\n",
    "\t\tlognorm_params = stats.lognorm.fit(data) # Log-Normal distribution (if all > 0)\n",
    "\t\treturn norm_params, exp_params, lognorm_params, True\n",
    "\telse:\n",
    "\t\treturn norm_params, exp_params, None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = 4\n",
    "num_rows = int(np.ceil(len(numeric_columns) / num_cols))\n",
    "plt.figure(figsize=(num_cols * 4, num_rows * 4))\n",
    "\n",
    "for i, col in enumerate(numeric_columns, 1):\n",
    "\tdata = df[col]\n",
    "\tnorm_params, exp_params, lognorm_params, plot_lognorm = fit_distributions(data)\n",
    "\tplt.subplot(num_rows, num_cols, i)\n",
    "\tsns.histplot(data, kde=True, bins=20, color='blue', stat=\"density\")\n",
    "\txmin, xmax = plt.xlim()\n",
    "\tx = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "\tp_norm = stats.norm.pdf(x, *norm_params) # Normal distribution\n",
    "\tplt.plot(x, p_norm, 'k-', label=f'Normal fit')\n",
    "\n",
    "\tp_exp = stats.expon.pdf(x, *exp_params) # Exponential distribution\n",
    "\tplt.plot(x, p_exp, 'r-', label=f'Exponential fit')\n",
    "\n",
    "\tif plot_lognorm:\n",
    "\t\tp_lognorm = stats.lognorm.pdf(x, *lognorm_params) # Log-Normal distribution (if all > 0)\n",
    "\t\tplt.plot(x, p_lognorm, 'g-', label=f'Log-Normal fit') \n",
    "\n",
    "\tplt.title(f'Distribution of {col}')\n",
    "\tplt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "exponential_features = ['Yds.1', 'Yds.3']\n",
    "normal_features = [col for col in numeric_columns if col not in exponential_features and col not in ['Result', 'Season', 'Time', 'Week', 'G#']]\n",
    "\n",
    "\n",
    "normal_data = df[normal_features]\n",
    "exponential_data = df[exponential_features]\n",
    "\n",
    "normal_data_standardized = normal_data.apply(zscore)\n",
    "exponential_data_log = np.log1p(exponential_data)\n",
    "exponential_data_log_standardized = exponential_data_log.apply(zscore)\n",
    "\n",
    "normal_anomalies_3 = (np.abs(normal_data_standardized) > 3).any(axis=1)\n",
    "normal_anomalies_4 = (np.abs(normal_data_standardized) > 4).any(axis=1)\n",
    "exponential_anomalies_3 = (np.abs(exponential_data_log_standardized) > 3).any(axis=1)\n",
    "exponential_anomalies_4 = (np.abs(exponential_data_log_standardized) > 4).any(axis=1)\n",
    "final_anomalies_3 = normal_anomalies_3 | exponential_anomalies_3\n",
    "final_anomalies_4 = normal_anomalies_4 | exponential_anomalies_4\n",
    "\n",
    "print(f\"Anomalies detected (threshold = 3): {final_anomalies_3.mean() * 100:.2f}%\")\n",
    "print(f\"Anomalies detected (threshold = 4): {final_anomalies_4.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(normal_data_standardized.values.flatten(), bins=50, alpha=0.7, label=\"Normal Features\")\n",
    "plt.hist(exponential_data_log_standardized.values.flatten(), bins=50, alpha=0.7, label=\"Exponential Features\")\n",
    "plt.title(\"Distribution of Z-scores\")\n",
    "plt.xlabel(\"Z-score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.copy()\n",
    "temp_df['Anomaly'] = final_anomalies_4.astype(int)\n",
    "anomalous_rows = temp_df[temp_df['Anomaly'] == 1]\n",
    "anomalous_rows.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Correlation Analysis**\n",
    "\n",
    "Correlation analysis helps to guide out feature engineering. It is useful for detecting irrelavent information and reducing dimensionality. To do so, we did the following:\n",
    "- Correlation of each column with 'Result'\n",
    "\t- Result only had a high correlation with PtDif, which makes sense as that's what determines the winner\n",
    "\t- Inc and Int dont seem to be correlated to Result\n",
    "- Correlation of each column with every other column (threshold = 0.7)\n",
    "\t- There seems to be several stats which are very highly correlated to each other, mostly due to how they are calculated\n",
    "\t- 'G#' and 'Week': expected due to both being time based, but don't seem to be hihgly correlated to any stat nor the result\n",
    "\t- A lot of the *Y/A are hihgly coorrelated, since it's mostly calcualtions from the total yards\n",
    "\t- 'Rate' also seems to be highly correlated to multiple other stats, also due to how rate is calculated\n",
    "\t- There is also a pattern of overall stat and stat %, which are high correlated and may be redundant\n",
    "- Correlation of each column with 'Y/P'\n",
    "\t- NY/A, ANY/A, AY/A, Y/A, Y/C, and Yds are all highly coorelated with 'Y/P'\n",
    "- Correlation of each column with 'DY/P'\n",
    "\t- 'DY/P' does not seem to be highly coorelated to any stat\n",
    "- Correlation of each column with 'Rate'\n",
    "\t- 'Rate' is decently correlated to stats related to TD, cmps, yards, and ints, which are used to calculate it\n",
    "- Correlation of each column with 'Yds'\n",
    "\t- 'Yds' is hihgly correlated to stats such as Cmp and tot\n",
    "- Correlation of each column with 'Yds.1'\n",
    "\t- Yards lost to sacks is strongly correlated to the num of sacks \"Sk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "numeric_data = df[numeric_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute the correlation matrix\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Step 2: Mask the upper triangle to avoid duplicate correlations (i.e., A vs B and B vs A)\n",
    "import numpy as np\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Step 3: Extract pairs of features with strong correlations (e.g., |correlation| > 0.7)\n",
    "threshold = 0.7\n",
    "strong_correlations = correlation_matrix.where(mask).stack().reset_index()\n",
    "strong_correlations.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "\n",
    "# Step 4: Filter strong correlations above the threshold (both positive and negative)\n",
    "strong_correlations = strong_correlations[strong_correlations['Correlation'].abs() > threshold]\n",
    "\n",
    "# Step 5: Remove self-correlations (Feature 1 and Feature 2 being the same)\n",
    "strong_correlations = strong_correlations[strong_correlations['Feature 1'] != strong_correlations['Feature 2']]\n",
    "\n",
    "# Step 6: Sort the correlations by absolute value\n",
    "strong_correlations['abs_correlation'] = strong_correlations['Correlation'].abs()\n",
    "strong_correlations_sorted = strong_correlations.sort_values(by='abs_correlation', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Step 7: Display the result\n",
    "print(strong_correlations_sorted[['Feature 1', 'Feature 2', 'Correlation']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations_with_feature(feature_name):\n",
    "\tcorrelations = numeric_data.corrwith(df[feature_name])\n",
    "\tcorrelations = correlations.reset_index()\n",
    "\n",
    "\tcorrelations.columns = ['Feature', 'Correlation with ' + feature_name]\n",
    "\tcorrelations['abs_correlation'] = correlations['Correlation with ' + feature_name].abs()\n",
    "\n",
    "\tcorrelations_sorted = correlations.sort_values(by='abs_correlation', ascending=False).reset_index(drop=True)\n",
    "\tprint(correlations_sorted[['Feature', 'Correlation with ' + feature_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_with_feature('Result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_with_feature('Week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_with_feature('G#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_with_feature('Y/P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_with_feature('DY/P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_with_feature('Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_with_feature('Yds.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Selection & Engineering**\n",
    "\n",
    "Feature selection and engineering help to identify the most relevant variables (reduces dimentionality) and transform raw data into informative features, improving model accuracy, reducing complexity, and preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature Selection**\n",
    "\n",
    "For our feature selection, we used both the data from the correlation analysis as well as logic and previous experience with the sport. \n",
    "\n",
    "The general method is to train the model on each game with the format of<br>\n",
    "[General Game Data | Team 1 Stats| Team 2 Stats | Win/Loss]\n",
    "\n",
    "Since each game is presented twice in the data set (mirror for each team), we can simply get the data of team 1 for each game and then merge\n",
    "\n",
    "General Game Data:\n",
    "- Season\n",
    "- Date\n",
    "- Team\n",
    "- Opp\n",
    "- Away\n",
    "\n",
    "Win/Loss:\n",
    "- Result\n",
    "\n",
    "Team 1 Stats Included:\n",
    "- Pts: Generally important for a win\n",
    "- PtDif: How much they won/lost by\n",
    "- TO: Generally important for a win\n",
    "- Rate: Summarizes many important stats for a game (NFL rating)\n",
    "- Y/P: Generally important for a win, strongly correlated to other stats, serves as a summary\n",
    "- DY/P: Generally important for a win\n",
    "- ToP: Generally important for a win\n",
    "- Sk: Generally detrimental to a win\n",
    "- Yds.1: Sacked yards: Generally detrimental to a win\n",
    "- Att: Rushing attempts: generally a positive sign of performace\n",
    "- Rush_Yds: Generally important for a win\n",
    "\n",
    "Team 1 Stats Excluded:\n",
    "- Rate.1: A different rating system to the NFL rating, removed for consistency\n",
    "- NY/A: Strongly correlated to Y/P, redundant\n",
    "- ANY/A: Strongly correlated to Y/P, redundant\n",
    "- AY/A: Strongly correlated to Y/P, redundant\n",
    "- Y/A: Strongly correlated to Y/P, redundant\n",
    "- Y/C: Strongly correlated to Y/P, redundant\n",
    "- Cmp: Strongly correlated to Yds, redundant\n",
    "- Inc: reflected in Y/P\n",
    "- Int: reflected in DY/P\n",
    "- Rush_Y/A: reflected in Att and Rush_Yds\n",
    "- TD: Reflected in Pts\n",
    "- Yds: Reflected in Y/P\n",
    "- Att.2: Reflected in Y/P\n",
    "\n",
    "Team 2 Stats (All Excluded):\n",
    "- Att.3\n",
    "- Cmp%.1\n",
    "- Cmp.1\n",
    "- Int.1\n",
    "- Sk.1\n",
    "- Tot.1\n",
    "- TD.2\n",
    "- TD.3\n",
    "- Y/A.1\n",
    "- Yds.2\n",
    "- Yds.3\n",
    "- Opp_Rush_Yds\n",
    "- Att.1\n",
    "- TD.1\n",
    "- TO.1\n",
    "- PtsO\n",
    "\n",
    "Excluded:\n",
    "- Time: Irrelavent\n",
    "- Week: Irrelavent\n",
    "- G#: Irrelavent\n",
    "- Day: Irrelavent\n",
    "- Cmp%: Strongly correlated to Cmp, redundant\n",
    "- Int%: Strongly correlated to Int, redundant\n",
    "- Sk%: Strongly correlated to Sk, redundant\n",
    "- TD%: Strongly correlated to TD, redundant\n",
    "- Ply: Irrelavent, total for both teams\n",
    "- DPly: Irrelavent, total for both teams\n",
    "- PC: Irrelavent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, in the end we have:\n",
    "\n",
    "General Game Data:\n",
    "- Season -> Season\n",
    "- Date -> Date\n",
    "- Team -> Team1\n",
    "- Opp -> Team2\n",
    "- Away -> Home\n",
    "\n",
    "Win/Loss:\n",
    "- Result -> Result\n",
    "\n",
    "Team 1 Stats Included:\n",
    "- Pts -> Team1Pts\n",
    "- PtDif -> Team1PtDiff\n",
    "- TO -> Team1TM\n",
    "- Rate -> Team1Rating\n",
    "- Y/P -> Team1Y/P\n",
    "- DY/P -> Team1DY/P\n",
    "- ToP -> Team1ToP\n",
    "- Sk -> Team1Sks\n",
    "- Yds.1 -> Team1SkYds\n",
    "- Att -> Team1RushAtt\n",
    "- Rush_Yds -> Team1RushYds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have your original DataFrame 'df'\n",
    "\n",
    "# Create a new DataFrame with only the columns you want and rename them\n",
    "new_df = df[['Season', 'Date', 'Team', 'Opp', 'Away', 'Pts', 'PtDif', 'TO', 'Rate', 'Y/P', 'DY/P', 'ToP', 'Sk', 'Yds.1', 'Att', 'Rush_Yds', 'Result']].copy()\n",
    "\n",
    "# Rename columns\n",
    "new_df.rename(columns={\n",
    "    'Season': 'Season',\n",
    "    'Date': 'Date',\n",
    "    'Team': 'Team1',\n",
    "    'Opp': 'Team2',\n",
    "    'Away': 'Home',\n",
    "    'Pts': 'Team1Pts',\n",
    "    'PtDif': 'Team1PtDiff',\n",
    "    'TO': 'Team1TM',\n",
    "    'Rate': 'Team1Rating',\n",
    "    'Y/P': 'Team1Y/P',\n",
    "    'DY/P': 'Team1DY/P',\n",
    "    'ToP': 'Team1ToP',\n",
    "    'Sk': 'Team1Sks',\n",
    "    'Yds.1': 'Team1SkYds',\n",
    "    'Att': 'Team1RushAtt',\n",
    "    'Rush_Yds': 'Team1RushYds',\n",
    "    'Result': 'Result'\n",
    "}, inplace=True)\n",
    "\n",
    "# Now, 'new_df' will have the renamed columns as requested\n",
    "\n",
    "new_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Date</th>\n",
       "      <th>Season</th>\n",
       "      <th>Pts</th>\n",
       "      <th>PtsO</th>\n",
       "      <th>Rate</th>\n",
       "      <th>TO</th>\n",
       "      <th>Y/P</th>\n",
       "      <th>DY/P</th>\n",
       "      <th>ToP</th>\n",
       "      <th>Rate.1</th>\n",
       "      <th>Att</th>\n",
       "      <th>Att.1</th>\n",
       "      <th>Day</th>\n",
       "      <th>G#</th>\n",
       "      <th>Week</th>\n",
       "      <th>Away</th>\n",
       "      <th>Opp</th>\n",
       "      <th>Result</th>\n",
       "      <th>PtDif</th>\n",
       "      <th>PC</th>\n",
       "      <th>Cmp</th>\n",
       "      <th>Att.2</th>\n",
       "      <th>Inc</th>\n",
       "      <th>Cmp%</th>\n",
       "      <th>Yds</th>\n",
       "      <th>TD</th>\n",
       "      <th>Int</th>\n",
       "      <th>TD%</th>\n",
       "      <th>Int%</th>\n",
       "      <th>Sk</th>\n",
       "      <th>Yds.1</th>\n",
       "      <th>Sk%</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>NY/A</th>\n",
       "      <th>AY/A</th>\n",
       "      <th>ANY/A</th>\n",
       "      <th>Y/C</th>\n",
       "      <th>Rush_Yds</th>\n",
       "      <th>Rush_Y/A</th>\n",
       "      <th>TD.1</th>\n",
       "      <th>Tot</th>\n",
       "      <th>Ply</th>\n",
       "      <th>DPly</th>\n",
       "      <th>TO.1</th>\n",
       "      <th>Time</th>\n",
       "      <th>Cmp.1</th>\n",
       "      <th>Att.3</th>\n",
       "      <th>Cmp%.1</th>\n",
       "      <th>Yds.2</th>\n",
       "      <th>TD.2</th>\n",
       "      <th>Sk.1</th>\n",
       "      <th>Yds.3</th>\n",
       "      <th>Int.1</th>\n",
       "      <th>Opp_Rush_Yds</th>\n",
       "      <th>Y/A.1</th>\n",
       "      <th>TD.3</th>\n",
       "      <th>Rush</th>\n",
       "      <th>Pass</th>\n",
       "      <th>Tot.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DET</td>\n",
       "      <td>2024-12-05</td>\n",
       "      <td>2024</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>109.7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.14</td>\n",
       "      <td>6.62</td>\n",
       "      <td>2166</td>\n",
       "      <td>110.2</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>Thu</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>GNB</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>78.0</td>\n",
       "      <td>280</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.38</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.67</td>\n",
       "      <td>7.20</td>\n",
       "      <td>7.02</td>\n",
       "      <td>8.8</td>\n",
       "      <td>111</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1</td>\n",
       "      <td>391</td>\n",
       "      <td>76</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>60.0</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>81</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GNB</td>\n",
       "      <td>2024-12-05</td>\n",
       "      <td>2024</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>111.7</td>\n",
       "      <td>0</td>\n",
       "      <td>6.62</td>\n",
       "      <td>5.14</td>\n",
       "      <td>1434</td>\n",
       "      <td>109.3</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "      <td>Thu</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>DET</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>65</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4.76</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.48</td>\n",
       "      <td>10.95</td>\n",
       "      <td>10.43</td>\n",
       "      <td>16.6</td>\n",
       "      <td>99</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3</td>\n",
       "      <td>298</td>\n",
       "      <td>45</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>41</td>\n",
       "      <td>78.0</td>\n",
       "      <td>280</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1</td>\n",
       "      <td>-12</td>\n",
       "      <td>-81</td>\n",
       "      <td>-93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEN</td>\n",
       "      <td>2024-12-02</td>\n",
       "      <td>2024</td>\n",
       "      <td>41</td>\n",
       "      <td>32</td>\n",
       "      <td>65.7</td>\n",
       "      <td>1</td>\n",
       "      <td>6.56</td>\n",
       "      <td>6.57</td>\n",
       "      <td>1670</td>\n",
       "      <td>86.5</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>Mon</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>CLE</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>73</td>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>51.4</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.40</td>\n",
       "      <td>6.40</td>\n",
       "      <td>6.40</td>\n",
       "      <td>16.3</td>\n",
       "      <td>106</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>61</td>\n",
       "      <td>84</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>58</td>\n",
       "      <td>58.6</td>\n",
       "      <td>475</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>-181</td>\n",
       "      <td>-152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CLE</td>\n",
       "      <td>2024-12-02</td>\n",
       "      <td>2024</td>\n",
       "      <td>32</td>\n",
       "      <td>41</td>\n",
       "      <td>88.1</td>\n",
       "      <td>-1</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.56</td>\n",
       "      <td>1930</td>\n",
       "      <td>65.7</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>Mon</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>DEN</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>73</td>\n",
       "      <td>34</td>\n",
       "      <td>58</td>\n",
       "      <td>24</td>\n",
       "      <td>58.6</td>\n",
       "      <td>475</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>4.92</td>\n",
       "      <td>8.2</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.89</td>\n",
       "      <td>14.0</td>\n",
       "      <td>77</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0</td>\n",
       "      <td>552</td>\n",
       "      <td>84</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>51.4</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2</td>\n",
       "      <td>-29</td>\n",
       "      <td>181</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JAX</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>2024</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>5.57</td>\n",
       "      <td>5.34</td>\n",
       "      <td>1663</td>\n",
       "      <td>92.5</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>Sun</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>HOU</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>18</td>\n",
       "      <td>57.1</td>\n",
       "      <td>276</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.45</td>\n",
       "      <td>6.45</td>\n",
       "      <td>11.5</td>\n",
       "      <td>97</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0</td>\n",
       "      <td>373</td>\n",
       "      <td>67</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>34</td>\n",
       "      <td>64.7</td>\n",
       "      <td>218</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1</td>\n",
       "      <td>-11</td>\n",
       "      <td>58</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Team       Date  Season  Pts  PtsO   Rate  TO   Y/P  DY/P   ToP  Rate.1  \\\n",
       "0  DET 2024-12-05    2024   34    31  109.7   0  5.14  6.62  2166   110.2   \n",
       "1  GNB 2024-12-05    2024   31    34  111.7   0  6.62  5.14  1434   109.3   \n",
       "2  DEN 2024-12-02    2024   41    32   65.7   1  6.56  6.57  1670    86.5   \n",
       "3  CLE 2024-12-02    2024   32    41   88.1  -1  6.57  6.56  1930    65.7   \n",
       "4  JAX 2024-12-01    2024   20    23   83.0  -1  5.57  5.34  1663    92.5   \n",
       "\n",
       "   Att  Att.1  Day  G#  Week  Away  Opp  Result  PtDif  PC  Cmp  Att.2  Inc  \\\n",
       "0   34     24  Thu  13    14     1  GNB       1      3  65   32     41    9   \n",
       "1   24     34  Thu  13    14     0  DET       0     -3  65   12     20    8   \n",
       "2   26     23  Mon  13    13     1  CLE       1      9  73   18     35   17   \n",
       "3   23     26  Mon  12    13     0  DEN       0     -9  73   34     58   24   \n",
       "4   25     25  Sun  12    13     1  HOU       0     -3  43   24     42   18   \n",
       "\n",
       "   Cmp%  Yds  TD  Int  TD%  Int%  Sk  Yds.1   Sk%   Y/A  NY/A   AY/A  ANY/A  \\\n",
       "0  78.0  280   3    1  7.3   2.4   1      3  2.38   6.8  6.67   7.20   7.02   \n",
       "1  60.0  199   1    0  5.0   0.0   1      7  4.76  10.0  9.48  10.95  10.43   \n",
       "2  51.4  294   1    2  2.9   5.7   0      0  0.00   8.4  8.40   6.40   6.40   \n",
       "3  58.6  475   4    3  6.9   5.2   3     22  4.92   8.2  7.79   7.24   6.89   \n",
       "4  57.1  276   2    1  4.8   2.4   0      0  0.00   6.6  6.57   6.45   6.45   \n",
       "\n",
       "    Y/C  Rush_Yds  Rush_Y/A  TD.1  Tot  Ply  DPly  TO.1  Time  Cmp.1  Att.3  \\\n",
       "0   8.8       111       3.3     1  391   76    45     1     3     12     20   \n",
       "1  16.6        99       4.1     3  298   45    76     1     3     32     41   \n",
       "2  16.3       106       4.1     2  400   61    84     2     3     34     58   \n",
       "3  14.0        77       3.3     0  552   84    61     3     3     18     35   \n",
       "4  11.5        97       3.9     0  373   67    61     1     3     22     34   \n",
       "\n",
       "   Cmp%.1  Yds.2  TD.2  Sk.1  Yds.3  Int.1  Opp_Rush_Yds  Y/A.1  TD.3  Rush  \\\n",
       "0    60.0    199     1     1      7      0            99    4.1     3    12   \n",
       "1    78.0    280     3     1      3      1           111    3.3     1   -12   \n",
       "2    58.6    475     4     3     22      3            77    3.3     0    29   \n",
       "3    51.4    294     1     0      0      2           106    4.1     2   -29   \n",
       "4    64.7    218     1     2     24      0           108    4.3     1   -11   \n",
       "\n",
       "   Pass  Tot.1  \n",
       "0    81     93  \n",
       "1   -81    -93  \n",
       "2  -181   -152  \n",
       "3   181    152  \n",
       "4    58     47  "
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Engineering**\n",
    "\n",
    "Currently each row only tells the story of the stats of each team, they do not however, properly tell the difference between the teams besides PtDif, therefore, we will introduce 3 new features:\n",
    "- Pass: passing margin\n",
    "- Rush: rushing margin\n",
    "- Yardage: yardage margin\n",
    "\n",
    "These help tell the 'story' of how close the match really was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Att' is a duplicate of column 'Rush_Att'\n",
      "Column 'Att.1' is a duplicate of column 'Opp_Rush_Att'\n",
      "Column 'DY/P' is a duplicate of column 'DY/P.1'\n",
      "Column 'Pts' is a duplicate of column 'Pts.1'\n",
      "Column 'PtsO' is a duplicate of column 'PtsO.1'\n",
      "Column 'Rate' is a duplicate of column 'Rate.2'\n",
      "Column 'Rate.1' is a duplicate of column 'Rate.3'\n",
      "Column 'TO' is a duplicate of column 'TO.2'\n",
      "Column 'ToP' is a duplicate of column 'ToP.1'\n",
      "Column 'Y/P' is a duplicate of column 'Y/P.1'\n",
      "                    0        1        2        3        4\n",
      "Att             34.00    24.00    26.00    23.00    25.00\n",
      "Att.1           24.00    34.00    23.00    26.00    25.00\n",
      "DY/P             6.62     5.14     6.57     6.56     5.34\n",
      "DY/P.1           6.62     5.14     6.57     6.56     5.34\n",
      "Opp_Rush_Att    24.00    34.00    23.00    26.00    25.00\n",
      "Pts             34.00    31.00    41.00    32.00    20.00\n",
      "Pts.1           34.00    31.00    41.00    32.00    20.00\n",
      "PtsO            31.00    34.00    32.00    41.00    23.00\n",
      "PtsO.1          31.00    34.00    32.00    41.00    23.00\n",
      "Rate           109.70   111.70    65.70    88.10    83.00\n",
      "Rate.1         110.20   109.30    86.50    65.70    92.50\n",
      "Rate.2         109.70   111.70    65.70    88.10    83.00\n",
      "Rate.3         110.20   109.30    86.50    65.70    92.50\n",
      "Rush_Att        34.00    24.00    26.00    23.00    25.00\n",
      "TO               0.00     0.00     1.00    -1.00    -1.00\n",
      "TO.2             0.00     0.00     1.00    -1.00    -1.00\n",
      "ToP           2166.00  1434.00  1670.00  1930.00  1663.00\n",
      "ToP.1         2166.00  1434.00  1670.00  1930.00  1663.00\n",
      "Y/P              5.14     6.62     6.56     6.57     5.57\n",
      "Y/P.1            5.14     6.62     6.56     6.57     5.57\n"
     ]
    }
   ],
   "source": [
    "# Columns with identical data\n",
    "duplicate_columns = {}\n",
    "seen_pairs = set()\n",
    "\n",
    "for col1 in df.columns:\n",
    "  for col2 in df.columns:\n",
    "    if col1 != col2 and df[col1].equals(df[col2]):\n",
    "      if (col1, col2) not in seen_pairs and (col2, col1) not in seen_pairs:\n",
    "        duplicate_columns[col1] = col2\n",
    "        seen_pairs.add((col1, col2))\n",
    "\n",
    "sorted_duplicate_columns = sorted(duplicate_columns.items())\n",
    "\n",
    "for col, duplicate in sorted_duplicate_columns:\n",
    "  print(f\"Column '{col}' is a duplicate of column '{duplicate}'\")\n",
    "\n",
    "duplicated_columns_list = list(set(duplicate_columns.keys()).union(duplicate_columns.values()))\n",
    "print(df[duplicated_columns_list].head(5).T.sort_index(axis=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
